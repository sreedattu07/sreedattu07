{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP1ZZiTMql9eTGmYYf92LVU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sreedattu07/sreedattu07/blob/main/Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOx53DqZgfgB",
        "outputId": "b98dac05-b9b3-4f13-ba89-85b3a8384ea7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ]
        }
      ],
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp /content/kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download imneonizer/normal-vs-camouflage-clothes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-orhubJgxtd",
        "outputId": "1537e3f7-e2ac-42b9-d1f9-2ea3823bb47f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading normal-vs-camouflage-clothes.zip to /content\n",
            "100% 1.82G/1.82G [01:19<00:00, 28.1MB/s]\n",
            "100% 1.82G/1.82G [01:19<00:00, 24.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -qq normal-vs-camouflage-clothes.zip"
      ],
      "metadata": {
        "id": "f30hfkZyhGMf"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import AveragePooling2D\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from sklearn.metrics import classification_report\n",
        "from matplotlib import pyplot as plt\n",
        "from imutils import paths\n",
        "import numpy as np\n",
        "import argparse\n",
        "import random\n",
        "import shutil\n",
        "import os"
      ],
      "metadata": {
        "id": "a_xtdgRRhWY7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plt_imshow(title, image):\n",
        "\t# convert the image frame BGR to RGB color space and display it\n",
        "\timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\tplt.imshow(image)\n",
        "\tplt.title(title)\n",
        "\tplt.grid(False)\n",
        "\tplt.show()"
      ],
      "metadata": {
        "id": "JUIevkBAhd_w"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    # initialize the path to the *original* input directory of images\n",
        "    ORIG_INPUT_DATASET = \"/content/8k_normal_vs_camouflage_clothes_images\"\n",
        "\n",
        "    # initialize the base path to the *new* directory that will contain\n",
        "    # our images after computing the training and testing split\n",
        "    BASE_PATH = \"/content/sample_data\"\n",
        "\n",
        "    # derive the training, validation, and testing directories\n",
        "    TRAIN_PATH = os.path.sep.join([BASE_PATH, \"training\"])\n",
        "    VAL_PATH = os.path.sep.join([BASE_PATH, \"validation\"])\n",
        "    TEST_PATH = os.path.sep.join([BASE_PATH, \"testing\"])\n",
        "\n",
        "    # define the amount of data that will be used training\n",
        "    TRAIN_SPLIT = 0.75\n",
        "\n",
        "    # the amount of validation data will be a percentage of the\n",
        "    # *training* data\n",
        "    VAL_SPLIT = 0.1\n",
        "\n",
        "    # define the names of the classes\n",
        "    CLASSES = [\"camouflage_clothes\", \"normal_clothes\"]\n",
        "\n",
        "    # initialize the initial learning rate, batch size, and number of\n",
        "    # epochs to train for\n",
        "    INIT_LR = 1e-4\n",
        "    BS = 32\n",
        "    NUM_EPOCHS = 3\n",
        "\n",
        "    # define the path to the serialized output model after training\n",
        "    MODEL_PATH = \"camo_detector.model\"\n",
        "\n",
        "# instantiate a Config object\n",
        "config = Config()"
      ],
      "metadata": {
        "id": "ZwD-cT4ahkE_"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# grab the paths to all input images in the original input directory\n",
        "# and shuffle them\n",
        "imagePaths = list(paths.list_images(config.ORIG_INPUT_DATASET))\n",
        "random.seed(42)\n",
        "random.shuffle(imagePaths)\n",
        "\n",
        "# compute the training and testing split\n",
        "i = int(len(imagePaths) * config.TRAIN_SPLIT)\n",
        "trainPaths = imagePaths[:i]\n",
        "testPaths = imagePaths[i:]\n",
        "\n",
        "# we'll be using part of the training data for validation\n",
        "i = int(len(trainPaths) * config.VAL_SPLIT)\n",
        "valPaths = trainPaths[:i]\n",
        "trainPaths = trainPaths[i:]\n",
        "\n",
        "# define the datasets that we'll be building\n",
        "datasets = [\n",
        "\t(\"training\", trainPaths, config.TRAIN_PATH),\n",
        "\t(\"validation\", valPaths, config.VAL_PATH),\n",
        "\t(\"testing\", testPaths, config.TEST_PATH)\n",
        "]"
      ],
      "metadata": {
        "id": "Qd0CVXs-h-aM"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for (dType, imagePaths, baseOutput) in datasets:\n",
        "\t# show which data split we are creating\n",
        "\tprint(\"[INFO] building '{}' split\".format(dType))\n",
        "\n",
        "\t# if the output base output directory does not exist, create it\n",
        "\tif not os.path.exists(baseOutput):\n",
        "\t\tprint(\"[INFO] 'creating {}' directory\".format(baseOutput))\n",
        "\t\tos.makedirs(baseOutput)\n",
        "\n",
        "\t# loop over the input image paths\n",
        "\tfor inputPath in imagePaths:\n",
        "\t\t# extract the filename of the input image along with its\n",
        "\t\t# corresponding class label\n",
        "\t\tfilename = inputPath.split(os.path.sep)[-1]\n",
        "\t\tlabel = inputPath.split(os.path.sep)[-2]\n",
        "\n",
        "\t\t# build the path to the label directory\n",
        "\t\tlabelPath = os.path.sep.join([baseOutput, label])\n",
        "\n",
        "\t\t# if the label output directory does not exist, create it\n",
        "\t\tif not os.path.exists(labelPath):\n",
        "\t\t\tprint(\"[INFO] 'creating {}' directory\".format(labelPath))\n",
        "\t\t\tos.makedirs(labelPath)\n",
        "\n",
        "\t\t# construct the path to the destination image and then copy\n",
        "\t\t# the image itself\n",
        "\t\tp = os.path.sep.join([labelPath, filename])\n",
        "\t\tshutil.copy2(inputPath, p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7utWobNiFHx",
        "outputId": "156fe450-9587-491e-a939-eaacae39cb02"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] building 'training' split\n",
            "[INFO] 'creating /content/sample_data/training/normal_clothes' directory\n",
            "[INFO] 'creating /content/sample_data/training/camouflage_clothes' directory\n",
            "[INFO] building 'validation' split\n",
            "[INFO] 'creating /content/sample_data/validation/camouflage_clothes' directory\n",
            "[INFO] 'creating /content/sample_data/validation/normal_clothes' directory\n",
            "[INFO] building 'testing' split\n",
            "[INFO] 'creating /content/sample_data/testing/normal_clothes' directory\n",
            "[INFO] 'creating /content/sample_data/testing/camouflage_clothes' directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# construct the argument parser and parse the arguments\n",
        "#ap = argparse.ArgumentParser()\n",
        "#ap.add_argument(\"-p\", \"--plot\", type=str, default=\"plot.png\",\n",
        "#\thelp=\"path to output loss/accuracy plot\")\n",
        "#args = vars(ap.parse_args())\n",
        "\n",
        "# since we are using Jupyter Notebooks we can replace our argument\n",
        "# parsing code with *hard coded* arguments and values\n",
        "args = {\n",
        "\t\"plot\": \"plot.png\"\n",
        "}\n",
        "\n",
        "# determine the total number of image paths in training, validation,\n",
        "# and testing directories\n",
        "totalTrain = len(list(paths.list_images(config.TRAIN_PATH)))\n",
        "totalVal = len(list(paths.list_images(config.VAL_PATH)))\n",
        "totalTest = len(list(paths.list_images(config.TEST_PATH)))"
      ],
      "metadata": {
        "id": "K_rA9C7GiUKS"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the training training data augmentation object\n",
        "trainAug = ImageDataGenerator(\n",
        "\trotation_range=25,\n",
        "\tzoom_range=0.1,\n",
        "\twidth_shift_range=0.1,\n",
        "\theight_shift_range=0.1,\n",
        "\tshear_range=0.2,\n",
        "\thorizontal_flip=True,\n",
        "\tfill_mode=\"nearest\")\n",
        "\n",
        "# initialize the validation/testing data augmentation object (which\n",
        "# we'll be adding mean subtraction to)\n",
        "valAug = ImageDataGenerator()\n",
        "\n",
        "# define the ImageNet mean subtraction (in RGB order) and set the\n",
        "# the mean subtraction value for each of the data augmentation\n",
        "# objects\n",
        "mean = np.array([123.68, 116.779, 103.939], dtype=\"float32\")\n",
        "trainAug.mean = mean\n",
        "valAug.mean = mean"
      ],
      "metadata": {
        "id": "MscMR5Z6ikIY"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the training generator\n",
        "trainGen = trainAug.flow_from_directory(\n",
        "\tconfig.TRAIN_PATH,\n",
        "\tclass_mode=\"categorical\",\n",
        "\ttarget_size=(224, 224),\n",
        "\tcolor_mode=\"rgb\",\n",
        "\tshuffle=True,\n",
        "\tbatch_size=config.BS)\n",
        "\n",
        "# initialize the validation generator\n",
        "valGen = valAug.flow_from_directory(\n",
        "\tconfig.VAL_PATH,\n",
        "\tclass_mode=\"categorical\",\n",
        "\ttarget_size=(224, 224),\n",
        "\tcolor_mode=\"rgb\",\n",
        "\tshuffle=False,\n",
        "\tbatch_size=config.BS)\n",
        "\n",
        "# initialize the testing generator\n",
        "testGen = valAug.flow_from_directory(\n",
        "\tconfig.TEST_PATH,\n",
        "\tclass_mode=\"categorical\",\n",
        "\ttarget_size=(224, 224),\n",
        "\tcolor_mode=\"rgb\",\n",
        "\tshuffle=False,\n",
        "\tbatch_size=config.BS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iNOAOmQipoY",
        "outputId": "108e899b-5adc-4864-c3d9-cf6d348f9a4c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 10731 images belonging to 2 classes.\n",
            "Found 1192 images belonging to 2 classes.\n",
            "Found 3975 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the ResNet-50 network, ensuring the head FC layer sets are left\n",
        "# off\n",
        "print(\"[INFO] preparing model...\")\n",
        "baseModel = ResNet50(weights=\"imagenet\", include_top=False,\n",
        "\tinput_tensor=Input(shape=(224, 224, 3)))\n",
        "\n",
        "# construct the head of the model that will be placed on top of the\n",
        "# the base model\n",
        "headModel = baseModel.output\n",
        "headModel = AveragePooling2D(pool_size=(7, 7))(headModel)\n",
        "headModel = Flatten(name=\"flatten\")(headModel)\n",
        "headModel = Dense(256, activation=\"relu\")(headModel)\n",
        "headModel = Dropout(0.5)(headModel)\n",
        "headModel = Dense(len(config.CLASSES), activation=\"softmax\")(headModel)\n",
        "\n",
        "# place the head FC model on top of the base model (this will become\n",
        "# the actual model we will train)\n",
        "model = Model(inputs=baseModel.input, outputs=headModel)\n",
        "\n",
        "# loop over all layers in the base model and freeze them so they will\n",
        "# *not* be updated during the training process\n",
        "for layer in baseModel.layers:\n",
        "\tlayer.trainable = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPR1kT_lixOJ",
        "outputId": "e0140f99-206b-45f1-ab5b-a80c33198a67"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] preparing model...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compile the model\n",
        "import tensorflow\n",
        "opt = tensorflow.keras.optimizers.legacy.Adam(learning_rate=config.INIT_LR, decay=config.INIT_LR / config.NUM_EPOCHS)\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=opt,\n",
        "\tmetrics=[\"accuracy\"])\n",
        "\n",
        "# train the model\n",
        "print(\"[INFO] training model...\")\n",
        "H = model.fit_generator(\n",
        "\ttrainGen,\n",
        "\tsteps_per_epoch=totalTrain // config.BS,\n",
        "\tvalidation_data=valGen,\n",
        "\tvalidation_steps=totalVal // config.BS,\n",
        "\tepochs=config.NUM_EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8TEGs9ji4RF",
        "outputId": "68eb488e-f343-4407-a168-309c781ff688"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] training model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-860bdb10b55e>:9: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  H = model.fit_generator(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "335/335 [==============================] - 2560s 8s/step - loss: 0.2041 - accuracy: 0.9244 - val_loss: 0.0847 - val_accuracy: 0.9704\n",
            "Epoch 2/3\n",
            "335/335 [==============================] - 2550s 8s/step - loss: 0.0876 - accuracy: 0.9718 - val_loss: 0.0668 - val_accuracy: 0.9780\n",
            "Epoch 3/3\n",
            "335/335 [==============================] - 2551s 8s/step - loss: 0.0721 - accuracy: 0.9760 - val_loss: 0.0656 - val_accuracy: 0.9764\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reset the testing generator and then use our trained model to\n",
        "# make predictions on the data\n",
        "print(\"[INFO] evaluating network...\")\n",
        "testGen.reset()\n",
        "predIdxs = model.predict(testGen,\n",
        "\tsteps=(totalTest // config.BS) + 1)\n",
        "\n",
        "# for each image in the testing set we need to find the index of the\n",
        "# label with corresponding largest predicted probability\n",
        "predIdxs = np.argmax(predIdxs, axis=1)\n",
        "\n",
        "# show a nicely formatted classification report\n",
        "print(classification_report(testGen.classes, predIdxs,\n",
        "\ttarget_names=testGen.class_indices.keys()))\n",
        "\n",
        "# serialize the model to disk\n",
        "print(\"[INFO] saving model...\")\n",
        "model.save(config.MODEL_PATH, save_format=\"h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anUXoLVOzUTb",
        "outputId": "f56dc520-08a3-42a3-9a9f-c11cfd24cb86"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] evaluating network...\n",
            "125/125 [==============================] - 806s 6s/step\n",
            "                    precision    recall  f1-score   support\n",
            "\n",
            "camouflage_clothes       0.99      0.97      0.98      2007\n",
            "    normal_clothes       0.97      0.99      0.98      1968\n",
            "\n",
            "          accuracy                           0.98      3975\n",
            "         macro avg       0.98      0.98      0.98      3975\n",
            "      weighted avg       0.98      0.98      0.98      3975\n",
            "\n",
            "[INFO] saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "def predict_camouflage(image_path, model):\n",
        "    # Load the image\n",
        "    img = cv2.imread(image_path)\n",
        "\n",
        "    # Resize the image to the expected shape\n",
        "    img = cv2.resize(img, (224, 224))\n",
        "\n",
        "    # Preprocess the image\n",
        "    img_array = np.expand_dims(img, axis=0)\n",
        "    img_array = tf.keras.applications.resnet50.preprocess_input(img_array)\n",
        "\n",
        "    # Predict the camouflage\n",
        "    prediction = model.predict(img_array)\n",
        "    return prediction[0][0]\n",
        "\n",
        "# Example usage\n",
        "result = predict_camouflage('/content/8k_normal_vs_camouflage_clothes_images/normal_clothes/0001.jpg',model)\n",
        "if result >= 0.5:\n",
        "     print(\"The image contains camouflage clothing.\")\n",
        "else:\n",
        "     print(\"The image does not contain camouflage clothing.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-xS6qL93weZ",
        "outputId": "fd304e4f-7249-41d3-c65c-7cdf2b43eb2d"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 289ms/step\n",
            "The image does not contain camouflage clothing.\n"
          ]
        }
      ]
    }
  ]
}